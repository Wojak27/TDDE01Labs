theta.return[a] = log(prod(probVector))/length(x)
}
return(theta.return)
}
logLikelyhoodBayes = function(x, theta, landa){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
probLanda = landa*exp(-landa*x)
probTot = prod(probVector)*prod(probLanda)
theta.return[a] = log(probTot)/length(x)
}
print(length(theta.return))
return(theta.return)
}
#plot data
#z = 1:length(data$Length)
#plot(z, sort(data$Length, decreasing = TRUE), col = "red")
#plot how theta varies, question 2
#with the higher amount of samples the model is more certain and can estimate results better.
theta = seq(0, 6, by=0.1)
result1 = logLikelyhood(my_data$Length, theta)
result2 = logLikelyhood(my_data$Length[1:6], theta)
resultBayes = logLikelyhoodBayes(my_data$Length, theta, 10)
print(max(result1))
print(max(result2))
print(max(resultBayes))
#plot how theta varies question 3
plot(theta, resultBayes)
plot(theta, result2, col="red", lwd=2)
plot(theta, result1, col="blue")
print(sample(result1, 50))
#the theta parameter decays faster with the entire set. this could mean that the first 6 values we take out from the
#array have higher average than the entire array of data based on the peak of the curve.
train.sorted = sort(train$Length, decreasing = FALSE)
#hist(train.sorted, freq=FALSE, breaks=50, col="red")
print(train)
barplot(train$Length)
summary(train)
library(Amelia)
library(kknn)
library(fitdistrplus)
library(logspline)
#data for the machine lifetime. Length: shows lifetime of a machine
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignment2/machines.xlsx", 1)
n=dim(my_data)[1]
id=sample(1:n, n)
#splitting data into training data and test data
train = my_data[id,]
sort(train$Length)
descdist(train$Length, discrete = FALSE)
#the exponential distribution fits perfectly with all of the graphs
fit.norm = fitdist(train$Length, "exp")
plot(fit.norm)
pfunction = function(theta, x){
returnVector = 1:length(x)
for(a in 1 : length(x)){
returnVector[a] = theta*exp(-theta*a)
}
return(returnVector)
}
#this is maximum log likelyhood funtion
logLikelyhood = function(x, theta){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
theta.return[a] = log(prod(probVector))/length(x)
}
return(theta.return)
}
logLikelyhoodBayes = function(x, theta, landa){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
probLanda = landa*exp(-landa*x)
probTot = prod(probVector)*prod(probLanda)
theta.return[a] = log(probTot)/length(x)
}
print(length(theta.return))
return(theta.return)
}
#plot data
#z = 1:length(data$Length)
#plot(z, sort(data$Length, decreasing = TRUE), col = "red")
#plot how theta varies, question 2
#with the higher amount of samples the model is more certain and can estimate results better.
theta = seq(0, 6, by=0.1)
result1 = logLikelyhood(my_data$Length, theta)
result2 = logLikelyhood(my_data$Length[1:6], theta)
resultBayes = logLikelyhoodBayes(my_data$Length, theta, 10)
print(max(result1))
print(max(result2))
print(max(resultBayes))
#plot how theta varies question 3
plot(theta, resultBayes)
plot(theta, result2, col="red", lwd=2)
plot(theta, result1, col="blue")
print(sample(pfunction(max(result1),1000), 50))
#the theta parameter decays faster with the entire set. this could mean that the first 6 values we take out from the
#array have higher average than the entire array of data based on the peak of the curve.
train.sorted = sort(train$Length, decreasing = FALSE)
#hist(train.sorted, freq=FALSE, breaks=50, col="red")
print(train)
barplot(train$Length)
summary(train)
library(Amelia)
library(kknn)
library(fitdistrplus)
library(logspline)
#data for the machine lifetime. Length: shows lifetime of a machine
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignment2/machines.xlsx", 1)
n=dim(my_data)[1]
id=sample(1:n, n)
#splitting data into training data and test data
train = my_data[id,]
sort(train$Length)
descdist(train$Length, discrete = FALSE)
#the exponential distribution fits perfectly with all of the graphs
fit.norm = fitdist(train$Length, "exp")
plot(fit.norm)
pfunction = function(theta, x){
returnVector = 1:x
for(a in 1 : x){
returnVector[a] = theta*exp(-theta*a)
}
return(returnVector)
}
#this is maximum log likelyhood funtion
logLikelyhood = function(x, theta){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
theta.return[a] = log(prod(probVector))/length(x)
}
return(theta.return)
}
logLikelyhoodBayes = function(x, theta, landa){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
probLanda = landa*exp(-landa*x)
probTot = prod(probVector)*prod(probLanda)
theta.return[a] = log(probTot)/length(x)
}
print(length(theta.return))
return(theta.return)
}
#plot data
#z = 1:length(data$Length)
#plot(z, sort(data$Length, decreasing = TRUE), col = "red")
#plot how theta varies, question 2
#with the higher amount of samples the model is more certain and can estimate results better.
theta = seq(0, 6, by=0.1)
result1 = logLikelyhood(my_data$Length, theta)
result2 = logLikelyhood(my_data$Length[1:6], theta)
resultBayes = logLikelyhoodBayes(my_data$Length, theta, 10)
print(max(result1))
print(max(result2))
print(max(resultBayes))
#plot how theta varies question 3
plot(theta, resultBayes)
plot(theta, result2, col="red", lwd=2)
plot(theta, result1, col="blue")
print(sample(pfunction(max(result1),1000), 50))
#the theta parameter decays faster with the entire set. this could mean that the first 6 values we take out from the
#array have higher average than the entire array of data based on the peak of the curve.
train.sorted = sort(train$Length, decreasing = FALSE)
#hist(train.sorted, freq=FALSE, breaks=50, col="red")
print(train)
barplot(train$Length)
summary(train)
library(Amelia)
library(kknn)
library(fitdistrplus)
library(logspline)
#data for the machine lifetime. Length: shows lifetime of a machine
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignment2/machines.xlsx", 1)
n=dim(my_data)[1]
id=sample(1:n, n)
#splitting data into training data and test data
train = my_data[id,]
sort(train$Length)
descdist(train$Length, discrete = FALSE)
#the exponential distribution fits perfectly with all of the graphs
fit.norm = fitdist(train$Length, "exp")
plot(fit.norm)
pfunction = function(theta, x){
returnVector = 1:x
for(a in 1 : x){
returnVector[a] = theta*exp(-theta*a)
}
return(returnVector)
}
#this is maximum log likelyhood funtion
logLikelyhood = function(x, theta){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
theta.return[a] = log(prod(probVector))/length(x)
}
return(theta.return)
}
logLikelyhoodBayes = function(x, theta, landa){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
probLanda = landa*exp(-landa*x)
probTot = prod(probVector)*prod(probLanda)
theta.return[a] = log(probTot)/length(x)
}
print(length(theta.return))
return(theta.return)
}
#plot data
#z = 1:length(data$Length)
#plot(z, sort(data$Length, decreasing = TRUE), col = "red")
#plot how theta varies, question 2
#with the higher amount of samples the model is more certain and can estimate results better.
theta = seq(0, 6, by=0.1)
result1 = logLikelyhood(my_data$Length, theta)
result2 = logLikelyhood(my_data$Length[1:6], theta)
resultBayes = logLikelyhoodBayes(my_data$Length, theta, 10)
print(max(result1))
print(max(result2))
print(max(resultBayes))
#plot how theta varies question 3
plot(theta, resultBayes)
plot(theta, result2, col="red", lwd=2)
plot(theta, result1, col="blue")
samples = sample(pfunction(max(result1),1000), 50)
#the theta parameter decays faster with the entire set. this could mean that the first 6 values we take out from the
#array have higher average than the entire array of data based on the peak of the curve.
train.sorted = sort(train$Length, decreasing = FALSE)
#hist(train.sorted, freq=FALSE, breaks=50, col="red")
hist(samples, freq=FALSE, breaks=50, col="red")
print(train)
barplot(train$Length)
summary(train)
library(Amelia)
library(kknn)
library(fitdistrplus)
library(logspline)
#data for the machine lifetime. Length: shows lifetime of a machine
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignment2/machines.xlsx", 1)
n=dim(my_data)[1]
id=sample(1:n, n)
#splitting data into training data and test data
train = my_data[id,]
sort(train$Length)
descdist(train$Length, discrete = FALSE)
#the exponential distribution fits perfectly with all of the graphs
fit.norm = fitdist(train$Length, "exp")
plot(fit.norm)
pfunction = function(theta, x){
returnVector = 1:x
for(a in 1 : x){
returnVector[a] = theta*exp(-theta*a)
}
return(returnVector)
}
#this is maximum log likelyhood funtion
logLikelyhood = function(x, theta){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
theta.return[a] = log(prod(probVector))/length(x)
}
return(theta.return)
}
logLikelyhoodBayes = function(x, theta, landa){
x = sort(x, decreasing = TRUE)
theta.return = 1:length(theta)
for(a in 1 : length(theta)){
probVector = theta[a]*exp(-theta[a]*x)
probLanda = landa*exp(-landa*x)
probTot = prod(probVector)*prod(probLanda)
theta.return[a] = log(probTot)/length(x)
}
print(length(theta.return))
return(theta.return)
}
#plot data
#z = 1:length(data$Length)
#plot(z, sort(data$Length, decreasing = TRUE), col = "red")
#plot how theta varies, question 2
#with the higher amount of samples the model is more certain and can estimate results better.
theta = seq(0, 6, by=0.1)
result1 = logLikelyhood(my_data$Length, theta)
result2 = logLikelyhood(my_data$Length[1:6], theta)
resultBayes = logLikelyhoodBayes(my_data$Length, theta, 10)
print(max(result1))
print(max(result2))
print(max(resultBayes))
#plot how theta varies question 3
plot(theta, resultBayes)
plot(theta, result2, col="red", lwd=2)
plot(theta, result1, col="blue")
samples = sample(pfunction(max(result1),100), 50)
#the theta parameter decays faster with the entire set. this could mean that the first 6 values we take out from the
#array have higher average than the entire array of data based on the peak of the curve.
train.sorted = sort(train$Length, decreasing = FALSE)
#hist(train.sorted, freq=FALSE, breaks=50, col="red")
hist(samples, freq=FALSE, breaks=50, col="red")
print(train)
barplot(train$Length)
summary(train)
#datavariable=read.table(file= file.choose(), header = T, sep=" ", na.strings=" ", stringsAsFactors=F)
library(xlsx)
library(Amelia)
library(kknn)
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignemnt1/spambase.xlsx", 1)
#missmap(data, main = "Missing values vs observed") #for plotting missing data
n=dim(my_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
#splitting data into training data and test data
train = my_data[id,]
test = my_data[-id,]
fit <- glm(Spam~.,family=binomial(link='logit'),data=train)
summary(fit)
results = predict(fit,test)
results = ifelse(results > 0.5,1,0) #Assignment 1.2
#results = ifelse(results > 0.9,1,0) #Assignment 1.3
misClasificError = mean(results != test$Spam)
Tlogreg = table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(results, labels=c("Pred not spam", "Pred spam")));
print(Tlogreg)
heatmap(Tlogreg)
#print(paste('Accuracy',1-misClasificError))
print(paste('Missclassification kknn',misClasificError))
#kknn
#https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/
modelknn <- kknn(Spam~., train, train, k=1)
modelknn1 <- kknn(Spam~., train, test, k=1)
fitkknn <- fitted(modelknn)
results1 = ifelse(fitkknn > 0.5,1,0)
#result1 <- floor(fitkknn + 0.5)
misClasificError1 = mean(result1 != test$Spam)
#print(paste('Accuracy kknn',1-misClasificError1))
print(paste('Missclassification kknn',misClasificError1))
#kknn table
Tknn =table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(result1, labels=c("Pred not spam", "Pred spam")));
print(Tknn)
#heatmap(Tknn)
#plotting logistic regression
#plot(Spam~., data=my_data, col="red4")
#lines(Spam~., test, col="green4", lwd=2)
#logistic regression with glm() and predict()
#logistic regression vs linear regression
#https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression
#logistic regression tutorial in r
#https://datascienceplus.com/perform-logistic-regression-in-r/
#for plotting missing values in the dataset
#library(Amelia)
#missmap(training.data.raw, main = "Missing values vs observed")
#datavariable=read.table(file= file.choose(), header = T, sep=" ", na.strings=" ", stringsAsFactors=F)
library(xlsx)
library(Amelia)
library(kknn)
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignemnt1/spambase.xlsx", 1)
#missmap(data, main = "Missing values vs observed") #for plotting missing data
n=dim(my_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
#splitting data into training data and test data
train = my_data[id,]
test = my_data[-id,]
fit <- glm(Spam~.,family=binomial(link='logit'),data=train)
summary(fit)
results = predict(fit,test)
results = ifelse(results > 0.5,1,0) #Assignment 1.2
#results = ifelse(results > 0.9,1,0) #Assignment 1.3
misClasificError = mean(results != test$Spam)
Tlogreg = table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(results, labels=c("Pred not spam", "Pred spam")));
print(Tlogreg)
heatmap(Tlogreg)
#print(paste('Accuracy',1-misClasificError))
print(paste('Missclassification kknn',misClasificError))
#kknn
#https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/
modelknn <- kknn(Spam~., train, train, k=1)
modelknn1 <- kknn(Spam~., train, test, k=1)
fitkknn <- fitted(modelknn)
result1 <- floor(fitkknn + 0.5)
results2 = ifelse(results1 > 0.5,1,0)
misClasificError1 = mean(result2 != test$Spam)
#print(paste('Accuracy kknn',1-misClasificError1))
print(paste('Missclassification kknn',misClasificError1))
#kknn table
Tknn =table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(result1, labels=c("Pred not spam", "Pred spam")));
print(Tknn)
#heatmap(Tknn)
#plotting logistic regression
#plot(Spam~., data=my_data, col="red4")
#lines(Spam~., test, col="green4", lwd=2)
#logistic regression with glm() and predict()
#logistic regression vs linear regression
#https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression
#logistic regression tutorial in r
#https://datascienceplus.com/perform-logistic-regression-in-r/
#for plotting missing values in the dataset
#library(Amelia)
#missmap(training.data.raw, main = "Missing values vs observed")
#datavariable=read.table(file= file.choose(), header = T, sep=" ", na.strings=" ", stringsAsFactors=F)
library(xlsx)
library(Amelia)
library(kknn)
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignemnt1/spambase.xlsx", 1)
#missmap(data, main = "Missing values vs observed") #for plotting missing data
n=dim(my_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
#splitting data into training data and test data
train = my_data[id,]
test = my_data[-id,]
fit <- glm(Spam~.,family=binomial(link='logit'),data=train)
summary(fit)
results = predict(fit,test)
results = ifelse(results > 0.5,1,0) #Assignment 1.2
#results = ifelse(results > 0.9,1,0) #Assignment 1.3
misClasificError = mean(results != test$Spam)
Tlogreg = table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(results, labels=c("Pred not spam", "Pred spam")));
print(Tlogreg)
heatmap(Tlogreg)
#print(paste('Accuracy',1-misClasificError))
print(paste('Missclassification kknn',misClasificError))
#kknn
#https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/
modelknn <- kknn(Spam~., train, train, k=1)
modelknn1 <- kknn(Spam~., train, test, k=1)
fitkknn <- fitted(modelknn)
result1 <- floor(fitkknn + 0.5)
misClasificError1 = mean(result1 != test$Spam)
#print(paste('Accuracy kknn',1-misClasificError1))
print(paste('Missclassification kknn',misClasificError1))
#kknn table
Tknn =table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(result1, labels=c("Pred not spam", "Pred spam")));
print(Tknn)
#heatmap(Tknn)
#plotting logistic regression
#plot(Spam~., data=my_data, col="red4")
#lines(Spam~., test, col="green4", lwd=2)
#logistic regression with glm() and predict()
#logistic regression vs linear regression
#https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression
#logistic regression tutorial in r
#https://datascienceplus.com/perform-logistic-regression-in-r/
#for plotting missing values in the dataset
#library(Amelia)
#missmap(training.data.raw, main = "Missing values vs observed")
#datavariable=read.table(file= file.choose(), header = T, sep=" ", na.strings=" ", stringsAsFactors=F)
library(xlsx)
library(Amelia)
library(kknn)
my_data = readxl::read_excel(path = "/Users/karolwojtulewicz/Google\ Drive/skola/TDDE01/Labs/Lab1/Assignemnt1/spambase.xlsx", 1)
#missmap(data, main = "Missing values vs observed") #for plotting missing data
n=dim(my_data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
#splitting data into training data and test data
train = my_data[id,]
test = my_data[-id,]
fit <- glm(Spam~.,family=binomial(link='logit'),data=train)
summary(fit)
results = predict(fit,test)
results = ifelse(results > 0.5,1,0) #Assignment 1.2
#results = ifelse(results > 0.9,1,0) #Assignment 1.3
misClasificError = mean(results != test$Spam)
Tlogreg = table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(results, labels=c("Pred not spam", "Pred spam")));
print(Tlogreg)
heatmap(Tlogreg)
#print(paste('Accuracy',1-misClasificError))
print(paste('Missclassification kknn',misClasificError))
#kknn
#https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/
modelknn <- kknn(Spam~., train, train, k=30)
modelknn1 <- kknn(Spam~., train, test, k=1)
fitkknn <- fitted(modelknn)
result1 <- floor(fitkknn + 0.5)
misClasificError1 = mean(result1 != test$Spam)
#print(paste('Accuracy kknn',1-misClasificError1))
print(paste('Missclassification kknn',misClasificError1))
#kknn table
Tknn =table(factor(test$Spam, labels=c("Actual not spam", "Actual spam")),
factor(result1, labels=c("Pred not spam", "Pred spam")));
print(Tknn)
#heatmap(Tknn)
#plotting logistic regression
#plot(Spam~., data=my_data, col="red4")
#lines(Spam~., test, col="green4", lwd=2)
#logistic regression with glm() and predict()
#logistic regression vs linear regression
#https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression
#logistic regression tutorial in r
#https://datascienceplus.com/perform-logistic-regression-in-r/
#for plotting missing values in the dataset
#library(Amelia)
#missmap(training.data.raw, main = "Missing values vs observed")
